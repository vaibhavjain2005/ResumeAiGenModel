{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "15Fci2qCO1dA5xvLEaTi3v_9DbRE67tHJ",
      "authorship_tag": "ABX9TyMpUvoW86tNliObnf7UI8ks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7aa6f0441fc14c4fad4fb63e501b2aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b401344b6acd40cbb115205f5f75620c",
              "IPY_MODEL_12721478842e4dc8803f2750b1d7aa72",
              "IPY_MODEL_dbb070816dc945dcb4c935b98d901dba"
            ],
            "layout": "IPY_MODEL_c6e4b770d4ba48cab46198613e1dbfcf"
          }
        },
        "b401344b6acd40cbb115205f5f75620c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b8467799c9d46dfbf7bbc439a38ab1d",
            "placeholder": "​",
            "style": "IPY_MODEL_f2940b9ebafa4619bf3915c4092fd68a",
            "value": "Map: 100%"
          }
        },
        "12721478842e4dc8803f2750b1d7aa72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79790d67a8df4f188cfa1405d5d5fe3c",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0020193eaa2c4998b9f1e75344dae8c9",
            "value": 400
          }
        },
        "dbb070816dc945dcb4c935b98d901dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11297f56855c43e684f5b7001bf8e69a",
            "placeholder": "​",
            "style": "IPY_MODEL_e0eacb934d6c41528a2afa46b41c2c60",
            "value": " 400/400 [00:00&lt;00:00, 1171.73 examples/s]"
          }
        },
        "c6e4b770d4ba48cab46198613e1dbfcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b8467799c9d46dfbf7bbc439a38ab1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2940b9ebafa4619bf3915c4092fd68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79790d67a8df4f188cfa1405d5d5fe3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0020193eaa2c4998b9f1e75344dae8c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11297f56855c43e684f5b7001bf8e69a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0eacb934d6c41528a2afa46b41c2c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de521f76d54e4f3496051adf86557209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0d4a214c6444a40899c6e3a70185f67",
              "IPY_MODEL_32a5a0ec6baf4bde9aee71de5227c4d7",
              "IPY_MODEL_db1b586e701f4db1aca0566a749146d4"
            ],
            "layout": "IPY_MODEL_7f2bba83af4b4c12b36720ca11114806"
          }
        },
        "a0d4a214c6444a40899c6e3a70185f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ecbb77180f4cefb49c1b0acff97ea5",
            "placeholder": "​",
            "style": "IPY_MODEL_446c7603c8144356ac3ab6a82665459a",
            "value": "Map: 100%"
          }
        },
        "32a5a0ec6baf4bde9aee71de5227c4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33558e9eba3645f7bd348e98cdbdeb92",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5bc593db9144159bff5d16a037b2010",
            "value": 100
          }
        },
        "db1b586e701f4db1aca0566a749146d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff3e904a3cf4061acc962fa6d4bcbca",
            "placeholder": "​",
            "style": "IPY_MODEL_07bc9afc6638438b8921edfad36078ed",
            "value": " 100/100 [00:00&lt;00:00, 873.45 examples/s]"
          }
        },
        "7f2bba83af4b4c12b36720ca11114806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ecbb77180f4cefb49c1b0acff97ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446c7603c8144356ac3ab6a82665459a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33558e9eba3645f7bd348e98cdbdeb92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5bc593db9144159bff5d16a037b2010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aff3e904a3cf4061acc962fa6d4bcbca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07bc9afc6638438b8921edfad36078ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhavjain2005/ResumeAiGenModel/blob/main/final_resume.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7aa6f0441fc14c4fad4fb63e501b2aad",
            "b401344b6acd40cbb115205f5f75620c",
            "12721478842e4dc8803f2750b1d7aa72",
            "dbb070816dc945dcb4c935b98d901dba",
            "c6e4b770d4ba48cab46198613e1dbfcf",
            "2b8467799c9d46dfbf7bbc439a38ab1d",
            "f2940b9ebafa4619bf3915c4092fd68a",
            "79790d67a8df4f188cfa1405d5d5fe3c",
            "0020193eaa2c4998b9f1e75344dae8c9",
            "11297f56855c43e684f5b7001bf8e69a",
            "e0eacb934d6c41528a2afa46b41c2c60",
            "de521f76d54e4f3496051adf86557209",
            "a0d4a214c6444a40899c6e3a70185f67",
            "32a5a0ec6baf4bde9aee71de5227c4d7",
            "db1b586e701f4db1aca0566a749146d4",
            "7f2bba83af4b4c12b36720ca11114806",
            "94ecbb77180f4cefb49c1b0acff97ea5",
            "446c7603c8144356ac3ab6a82665459a",
            "33558e9eba3645f7bd348e98cdbdeb92",
            "c5bc593db9144159bff5d16a037b2010",
            "aff3e904a3cf4061acc962fa6d4bcbca",
            "07bc9afc6638438b8921edfad36078ed"
          ]
        },
        "id": "ephOezGc2tf1",
        "outputId": "37def52c-acdb-44f1-b4b5-cf07ec975d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "T5 Resume Fine-Tuning Script (Stable)\n",
            "Optimized for Google Colab Free Tier\n",
            "======================================================================\n",
            "\n",
            "✓ Using device: cuda\n",
            "✓ GPU: Tesla T4\n",
            "✓ GPU Memory: 15.83 GB\n",
            "\n",
            "⚙️  MODEL: google/flan-t5-base\n",
            "✓ Max input length: 128 tokens\n",
            "✓ Max output length: 256 tokens\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Loading dataset...\n",
            "----------------------------------------------------------------------\n",
            "✓ Training examples: 400\n",
            "✓ Validation examples: 100\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Loading model and tokenizer...\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model parameters: 247.6M\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Tokenizing dataset...\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aa6f0441fc14c4fad4fb63e501b2aad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de521f76d54e4f3496051adf86557209"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Tokenization complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Configuring training (FP32 safe mode)...\n",
            "----------------------------------------------------------------------\n",
            "Training Configuration:\n",
            "  • Model: google/flan-t5-base\n",
            "  • Learning rate: 0.0002\n",
            "  • FP16: False (disabled for stability)\n",
            "  • Batch size: 2\n",
            "  • Gradient accumulation: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3331316582.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting training (FP32 mode)...\n",
            "======================================================================\n",
            "\n",
            "Tip: Checkpoints will save every 100 steps.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 09:30, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.835400</td>\n",
              "      <td>0.210916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.047194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.066000</td>\n",
              "      <td>0.043868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.044200</td>\n",
              "      <td>0.044552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "✓ Training completed successfully!\n",
            "======================================================================\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Saving final model...\n",
            "----------------------------------------------------------------------\n",
            "✓ Model saved to: ./flan-t5-base-resume-finetuned-stable/final_model\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Running final evaluation...\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  • eval_loss: 0.0439\n",
            "  • eval_runtime: 1.1090\n",
            "  • eval_samples_per_second: 90.1750\n",
            "  • eval_steps_per_second: 22.5440\n",
            "  • epoch: 8.0000\n",
            "\n",
            "======================================================================\n",
            "Training Quality Assessment:\n",
            "======================================================================\n",
            "✓ EXCELLENT: Model trained very well (loss < 0.5)\n",
            "\n",
            "Final validation loss: 0.0439\n",
            "======================================================================\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Quick Test Inference:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Input: Rewrite professionally: Built web applications using Python\n",
            "Output: Developed full-stack web platform leveraging Python and MySQL database that increased user engagement by 45% and reduced load times by 60%\n",
            "\n",
            "✓ All done! Your fine-tuned model is ready (FP32 stable mode).\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "T5 Fine-Tuning Script for Resume Generation (Stable Version)\n",
        "Optimized for Google Colab Free Tier\n",
        "- FP32 precision for stability\n",
        "- Gradient accumulation\n",
        "- Safe checkpoints and NaN protection\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import json\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from transformers import (\n",
        "        T5Tokenizer,\n",
        "        T5ForConditionalGeneration,\n",
        "        Trainer,\n",
        "        TrainingArguments,\n",
        "        DataCollatorForSeq2Seq\n",
        "    )\n",
        "    from datasets import Dataset\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ERROR: Missing libraries. Run this first:\")\n",
        "    print(\"!pip install --upgrade transformers datasets torch accelerate sentencepiece\")\n",
        "    print(\"Then restart runtime and re-run this cell.\")\n",
        "    print(\"=\"*70)\n",
        "    raise\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"T5 Resume Fine-Tuning Script (Stable)\")\n",
        "print(\"Optimized for Google Colab Free Tier\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# GPU Check\n",
        "# ------------------------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\n✓ Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------------------------------------------\n",
        "MODEL_NAME = \"google/flan-t5-base\"       # Stable for Colab Free Tier\n",
        "OUTPUT_DIR = \"./flan-t5-base-resume-finetuned-stable\"\n",
        "BATCH_SIZE = 2\n",
        "MAX_INPUT_LENGTH = 128\n",
        "MAX_OUTPUT_LENGTH = 256\n",
        "LEARNING_RATE = 2e-4  # safer and more stable\n",
        "\n",
        "print(f\"\\n⚙️  MODEL: {MODEL_NAME}\")\n",
        "print(f\"✓ Max input length: {MAX_INPUT_LENGTH} tokens\")\n",
        "print(f\"✓ Max output length: {MAX_OUTPUT_LENGTH} tokens\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD DATASETS\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Loading dataset...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "with open(\"train_data.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "with open(\"val_data.json\", \"r\") as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "print(f\"✓ Training examples: {len(train_data)}\")\n",
        "print(f\"✓ Validation examples: {len(val_data)}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD MODEL & TOKENIZER\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Loading model and tokenizer...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"✓ Model parameters: {model.num_parameters() / 1e6:.1f}M\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TOKENIZATION FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    targets = examples[\"output\"]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=MAX_OUTPUT_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Tokenizing dataset...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"input\", \"output\"]\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"input\", \"output\"]\n",
        ")\n",
        "\n",
        "print(\"✓ Tokenization complete\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DATA COLLATOR\n",
        "# ------------------------------------------------------------\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRAINING CONFIGURATION (Safe FP32 Mode)\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Configuring training (FP32 safe mode)...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "    gradient_accumulation_steps=8 // BATCH_SIZE,  # effective batch size = 8\n",
        "\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    fp16=False,  # <--- Disabled FP16 to fix NaN loss\n",
        "    fp16_full_eval=False,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    logging_steps=50,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_first_step=True,\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adafactor\",\n",
        "\n",
        "    dataloader_num_workers=0,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  • Model: {MODEL_NAME}\")\n",
        "print(f\"  • Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  • FP16: {training_args.fp16} (disabled for stability)\")\n",
        "print(f\"  • Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  • Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRAINING\n",
        "# ------------------------------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Starting training (FP32 mode)...\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nTip: Checkpoints will save every 100 steps.\\n\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ Training completed successfully!\")\n",
        "    print(\"=\"*70)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user. Checkpoint saved.\")\n",
        "except Exception as e:\n",
        "    print(\"\\n⚠ Training stopped due to error:\")\n",
        "    print(e)\n",
        "    print(\"You can resume from the last checkpoint by re-running trainer.train().\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# SAVE MODEL\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Saving final model...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "final_dir = f\"{OUTPUT_DIR}/final_model\"\n",
        "model.save_pretrained(final_dir)\n",
        "tokenizer.save_pretrained(final_dir)\n",
        "print(f\"✓ Model saved to: {final_dir}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EVALUATION\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Running final evaluation...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nValidation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"  • {key}: {value:.4f}\")\n",
        "\n",
        "final_loss = eval_results.get('eval_loss', 999)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Training Quality Assessment:\")\n",
        "print(\"=\"*70)\n",
        "if final_loss < 0.5:\n",
        "    print(\"✓ EXCELLENT: Model trained very well (loss < 0.5)\")\n",
        "elif final_loss < 1.0:\n",
        "    print(\"✓ GOOD: Model trained well (loss < 1.0)\")\n",
        "elif final_loss < 1.5:\n",
        "    print(\"⚠ FAIR: Somewhat trained (loss < 1.5)\")\n",
        "else:\n",
        "    print(\"❌ POOR: Model may need more epochs or tuning.\")\n",
        "print(f\"\\nFinal validation loss: {final_loss:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# QUICK TEST INFERENCE\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Quick Test Inference:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "test_input = \"Rewrite professionally: Built web applications using Python\"\n",
        "inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=MAX_OUTPUT_LENGTH,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nInput: {test_input}\")\n",
        "print(f\"Output: {result}\")\n",
        "\n",
        "print(\"\\n✓ All done! Your fine-tuned model is ready (FP32 stable mode).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test Script for Fine-Tuned T5 Resume Model\n",
        "\n",
        "Tests the model on various resume generation tasks.\n",
        "This script is specifically designed to load a model fine-tuned from\n",
        "Hugging Face's T5/FLAN-T5 architecture.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "# Suppress TensorFlow logging if used, though this script uses PyTorch\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    # We use T5 classes as Flan-T5 uses the same architecture and tokenization\n",
        "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ERROR: Failed to import transformers\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nError: {e}\\n\")\n",
        "    print(\"Please ensure the 'transformers' and 'torch' libraries are installed.\")\n",
        "    print(\"=\"*70)\n",
        "    raise\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"T5/Flan-T5 Resume Model Testing\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Configuration\n",
        "# IMPORTANT: This path MUST point to the directory where your fine-tuned model\n",
        "# (including pytorch_model.bin and tokenizer_config.json) is saved.\n",
        "MODEL_PATH = \"./flan-t5-base-resume-finetuned-stable/final_model\"\n",
        "MAX_INPUT_LENGTH = 128\n",
        "MAX_OUTPUT_LENGTH = 256\n",
        "\n",
        "# Check if model exists\n",
        "if not os.path.exists(MODEL_PATH) or not os.path.exists(os.path.join(MODEL_PATH, \"pytorch_model.bin\")):\n",
        "    print(f\"\\n❌ Error: Model files not found in the expected directory: {MODEL_PATH}\")\n",
        "    print(\"\\nPlease verify the path or run the training script first.\")\n",
        "    exit(1)\n",
        "\n",
        "# Load model\n",
        "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Use T5Tokenizer for loading Flan-T5 models\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"✓ Model loaded successfully\\n\")\n",
        "\n",
        "# Test examples\n",
        "test_cases = [\n",
        "    {\n",
        "        \"category\": \"Professional Summary (Software)\",\n",
        "        \"input\": \"Create professional summary: Software Engineering, 5 years, Python, AWS, Docker\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Professional Summary (Data Science)\",\n",
        "        \"input\": \"Create professional summary: Data Science, 7 years, Machine Learning, Python, TensorFlow\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Experience Bullet (Development)\",\n",
        "        \"input\": \"Rewrite professionally: Built web applications using Python, Django, PostgreSQL\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Experience Bullet (Analysis)\",\n",
        "        \"input\": \"Rewrite professionally: Analyzed data using Python, SQL, Tableau\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Experience Bullet (Management)\",\n",
        "        \"input\": \"Rewrite professionally: Managed team using Agile, Scrum\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Project Description (Web)\",\n",
        "        \"input\": \"Enhance project: E-commerce Platform using React, Node.js, MongoDB\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Project Description (ML)\",\n",
        "        \"input\": \"Enhance project: Machine Learning Model using Python, TensorFlow\"\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Running Test Cases\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    print(f\"\\n{'─'*70}\")\n",
        "    print(f\"Test {i}/{len(test_cases)}: {test['category']}\")\n",
        "    print(f\"{'─'*70}\")\n",
        "    print(f\"Input:\\n  {test['input']}\")\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        test['input'],\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate output\n",
        "    # Recommended generation parameters for T5-based models:\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=MAX_OUTPUT_LENGTH,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,\n",
        "            length_penalty=1.2\n",
        "        )\n",
        "\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"\\nOutput:\\n  {result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Testing Complete!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Interactive mode\n",
        "print(\"\\n\" + \"─\"*70)\n",
        "print(\"Interactive Mode\")\n",
        "print(\"─\"*70)\n",
        "print(\"\\nYou can now test your own prompts.\")\n",
        "print(\"Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter your prompt: \").strip()\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"\\nGoodbye!\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        user_input,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=MAX_OUTPUT_LENGTH,\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=3,\n",
        "                length_penalty=1.2\n",
        "            )\n",
        "\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"\\nGenerated:\\n{result}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during generation: {e}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NDJkUoqeBRK4",
        "outputId": "8a618cca-6a14-45e7-d75b-7dc75bc859db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "T5/Flan-T5 Resume Model Testing\n",
            "======================================================================\n",
            "\n",
            "❌ Error: Model files not found in the expected directory: ./flan-t5-base-resume-finetuned-stable/final_model\n",
            "\n",
            "Please verify the path or run the training script first.\n",
            "\n",
            "Loading model from: ./flan-t5-base-resume-finetuned-stable/final_model\n",
            "Using device: cuda\n",
            "\n",
            "✓ Model loaded successfully\n",
            "\n",
            "======================================================================\n",
            "Running Test Cases\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 1/7: Professional Summary (Software)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Create professional summary: Software Engineering, 5 years, Python, AWS, Docker\n",
            "\n",
            "Output:\n",
            "  Results-driven Software Engineering professional with 5+ years of experience leveraging Python and AWS to deliver innovative solutions. Proven track record of improving efficiency and driving business outcomes through technical expertise.\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 2/7: Professional Summary (Data Science)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Create professional summary: Data Science, 7 years, Machine Learning, Python, TensorFlow\n",
            "\n",
            "Output:\n",
            "  Results-driven Data Science professional with 7+ years of experience leveraging Machine Learning and Python to deliver innovative solutions. Proven track record of improving efficiency and driving business outcomes through technical expertise.\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 3/7: Experience Bullet (Development)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Rewrite professionally: Built web applications using Python, Django, PostgreSQL\n",
            "\n",
            "Output:\n",
            "  Developed full-stack web platform leveraging Python and Django that increased user engagement by 45% and reduced load times by 60%\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 4/7: Experience Bullet (Analysis)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Rewrite professionally: Analyzed data using Python, SQL, Tableau\n",
            "\n",
            "Output:\n",
            "  Analyzed large-scale datasets with Python, SQL, Tableau to identify trends, resulting in data-driven strategies that increased conversion rates by 30%\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 5/7: Experience Bullet (Management)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Rewrite professionally: Managed team using Agile, Scrum\n",
            "\n",
            "Output:\n",
            "  Directed team operations for 10-member engineering group, establishing best practices that enhanced code quality and team productivity by 40%\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 6/7: Project Description (Web)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Enhance project: E-commerce Platform using React, Node.js, MongoDB\n",
            "\n",
            "Output:\n",
            "  Developed full-stack e-commerce platform with React frontend and Node.js backend, integrating MongoDB payment processing for seamless transactions. Implemented user authentication, product catalog with search/filter functionality, and shopping cart system. Achieved 99.9% uptime and processed $100K+ in monthly transactions.\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Test 7/7: Project Description (ML)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Input:\n",
            "  Enhance project: Machine Learning Model using Python, TensorFlow\n",
            "\n",
            "Output:\n",
            "  Developed predictive machine learning model using Python and TensorFlow to forecast customer churn with 89% accuracy. Performed data preprocessing, feature engineering, and hyperparameter tuning. Deployed model to production, enabling proactive retention strategies that reduced chURn by 22%.\n",
            "\n",
            "======================================================================\n",
            "Testing Complete!\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Interactive Mode\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "You can now test your own prompts.\n",
            "Type 'quit' to exit.\n",
            "\n",
            "Enter your prompt: hi\n",
            "\n",
            "Generated:\n",
            "hey, how are you? i'm fine. i have a question.\n",
            "\n",
            "Enter your prompt: okay \n",
            "\n",
            "Generated:\n",
            "ok, okay\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4076404032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your prompt: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install dependencies (run this in a cell first)\n",
        "\n",
        "!pip install flask pyngrok transformers torch sentencepiece reportlab -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p4DXjd8K7-w",
        "outputId": "bf41e6a3-c494-4ca1-8c57-758966d3746c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import io\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
        "from reportlab.lib.enums import TA_LEFT, TA_CENTER\n",
        "from reportlab.lib import colors\n",
        "from datetime import datetime\n",
        "import json"
      ],
      "metadata": {
        "id": "gOa4n_l8M-Gp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# STEP 2: Load the improved model\n",
        "print(\"Loading FLAN-T5-Large model... (better quality, still free)\")\n",
        "model_path = \"./flan-t5-base-resume-finetuned-stable/final_model\"  # Upgraded from base for better results\n",
        "\n",
        "# Use T5Tokenizer and T5ForConditionalGeneration directly\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A_2faxsM_ZF",
        "outputId": "8fe14361-e914-486f-fdb1-5da0eaa46147"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FLAN-T5-Large model... (better quality, still free)\n",
            "✅ Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #AI Functions for Resume Generation\n",
        "def extract_keywords_from_job(job_description):\n",
        "    \"\"\"Extract important keywords from job description\"\"\"\n",
        "    prompt = f\"\"\"Extract key skills and technologies from this job posting. List only the most important ones as comma-separated keywords.\n",
        "\n",
        "Job Description: {job_description}\n",
        "\n",
        "Important keywords:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_beams=4,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    keywords = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return keywords.strip()\n",
        "\n",
        "\n",
        "def tailor_resume_experience(experience_text, keywords):\n",
        "    \"\"\"Rewrite experience bullet to match job requirements\"\"\"\n",
        "    prompt = f\"\"\"Rewrite this work experience bullet point to be professional and impactful. Use action verbs and be specific.\n",
        "\n",
        "Original: {experience_text}\n",
        "\n",
        "Improved bullet point:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_beams=4,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    improved = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # If the output is too similar to input or just keywords, return improved original\n",
        "    if len(improved) < 10 or improved == keywords:\n",
        "        return experience_text\n",
        "    return improved.strip()\n",
        "\n",
        "\n",
        "def tailor_project_description(project_desc, project_tech, keywords):\n",
        "    \"\"\"Enhance project description to highlight relevant skills\"\"\"\n",
        "    prompt = f\"\"\"Rewrite this project description to be more professional and highlight technical achievements.\n",
        "\n",
        "Project: {project_desc}\n",
        "Technologies: {project_tech}\n",
        "\n",
        "Enhanced description:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_beams=4,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    improved = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # If output is too short or generic, return original\n",
        "    if len(improved) < 15:\n",
        "        return project_desc\n",
        "    return improved.strip()\n",
        "\n",
        "\n",
        "def generate_professional_summary(user_data, keywords, job_description):\n",
        "    \"\"\"Generate a professional summary tailored to the job\"\"\"\n",
        "    # Get actual experience details\n",
        "    exp_count = len(user_data.get('experience', []))\n",
        "    skills = user_data.get('skills', [])[:6]\n",
        "\n",
        "    # Get first job title if available\n",
        "    first_role = \"Professional\"\n",
        "    if user_data.get('experience') and len(user_data['experience']) > 0:\n",
        "        first_role = user_data['experience'][0].get('title', 'Professional')\n",
        "\n",
        "    prompt = f\"\"\"Write a professional resume summary for someone with these qualifications:\n",
        "Role: {first_role}\n",
        "Years of experience: {exp_count} positions\n",
        "Skills: {', '.join(skills)}\n",
        "\n",
        "Write a 2-3 sentence professional summary:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=120,\n",
        "        num_beams=4,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        length_penalty=1.0\n",
        "    )\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Fallback if AI generates poor summary\n",
        "    if len(summary) < 30 or \"job requirements\" in summary.lower():\n",
        "        summary = f\"Results-driven {first_role} with experience in {', '.join(skills[:3])}. Proven track record of delivering high-quality solutions and collaborating with cross-functional teams.\"\n",
        "\n",
        "    return summary.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "u8EeBqATPMRF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF Generation Functions\n",
        "def create_professional_resume_pdf(resume_data):\n",
        "    \"\"\"Generate a professional, ATS-friendly PDF resume\"\"\"\n",
        "    buffer = io.BytesIO()\n",
        "    doc = SimpleDocTemplate(buffer, pagesize=letter,\n",
        "                           rightMargin=0.75*inch, leftMargin=0.75*inch,\n",
        "                           topMargin=0.75*inch, bottomMargin=0.75*inch)\n",
        "\n",
        "    story = []\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Custom styles\n",
        "    title_style = ParagraphStyle(\n",
        "        'CustomTitle',\n",
        "        parent=styles['Heading1'],\n",
        "        fontSize=24,\n",
        "        textColor=colors.HexColor('#2C3E50'),\n",
        "        spaceAfter=6,\n",
        "        alignment=TA_CENTER,\n",
        "        fontName='Helvetica-Bold'\n",
        "    )\n",
        "\n",
        "    contact_style = ParagraphStyle(\n",
        "        'Contact',\n",
        "        parent=styles['Normal'],\n",
        "        fontSize=10,\n",
        "        textColor=colors.HexColor('#34495E'),\n",
        "        alignment=TA_CENTER,\n",
        "        spaceAfter=20\n",
        "    )\n",
        "\n",
        "    section_header_style = ParagraphStyle(\n",
        "        'SectionHeader',\n",
        "        parent=styles['Heading2'],\n",
        "        fontSize=14,\n",
        "        textColor=colors.HexColor('#2C3E50'),\n",
        "        spaceAfter=12,\n",
        "        spaceBefore=12,\n",
        "        fontName='Helvetica-Bold',\n",
        "        borderWidth=1,\n",
        "        borderColor=colors.HexColor('#3498DB'),\n",
        "        borderPadding=5,\n",
        "        backColor=colors.HexColor('#ECF0F1')\n",
        "    )\n",
        "\n",
        "    body_style = ParagraphStyle(\n",
        "        'CustomBody',\n",
        "        parent=styles['Normal'],\n",
        "        fontSize=10,\n",
        "        textColor=colors.HexColor('#2C3E50'),\n",
        "        spaceAfter=6,\n",
        "        leading=14\n",
        "    )\n",
        "\n",
        "    # Personal Information\n",
        "    personal = resume_data.get('personal_info', {})\n",
        "    story.append(Paragraph(personal.get('name', 'Your Name'), title_style))\n",
        "\n",
        "    contact_info = f\"{personal.get('email', '')} | {personal.get('phone', '')} | {personal.get('location', '')}\"\n",
        "    if personal.get('linkedin'):\n",
        "        contact_info += f\" | LinkedIn: {personal.get('linkedin')}\"\n",
        "    story.append(Paragraph(contact_info, contact_style))\n",
        "\n",
        "    # Professional Summary\n",
        "    if resume_data.get('professional_summary'):\n",
        "        story.append(Paragraph(\"PROFESSIONAL SUMMARY\", section_header_style))\n",
        "        story.append(Paragraph(resume_data['professional_summary'], body_style))\n",
        "        story.append(Spacer(1, 0.2*inch))\n",
        "\n",
        "    # Skills\n",
        "    if resume_data.get('skills'):\n",
        "        story.append(Paragraph(\"SKILLS\", section_header_style))\n",
        "        skills_text = \" • \".join(resume_data['skills'])\n",
        "        story.append(Paragraph(skills_text, body_style))\n",
        "        story.append(Spacer(1, 0.2*inch))\n",
        "\n",
        "    # Experience\n",
        "    if resume_data.get('experience'):\n",
        "        story.append(Paragraph(\"PROFESSIONAL EXPERIENCE\", section_header_style))\n",
        "        for exp in resume_data['experience']:\n",
        "            job_title = f\"<b>{exp.get('title', 'Position')}</b> | {exp.get('company', 'Company')}\"\n",
        "            story.append(Paragraph(job_title, body_style))\n",
        "\n",
        "            duration = f\"<i>{exp.get('start_date', '')} - {exp.get('end_date', 'Present')} | {exp.get('location', '')}</i>\"\n",
        "            story.append(Paragraph(duration, body_style))\n",
        "            story.append(Spacer(1, 0.1*inch))\n",
        "\n",
        "            for bullet in exp.get('bullets', []):\n",
        "                bullet_text = f\"• {bullet}\"\n",
        "                story.append(Paragraph(bullet_text, body_style))\n",
        "\n",
        "            story.append(Spacer(1, 0.15*inch))\n",
        "\n",
        "    # Projects\n",
        "    if resume_data.get('projects'):\n",
        "        story.append(Paragraph(\"PROJECTS\", section_header_style))\n",
        "        for project in resume_data['projects']:\n",
        "            project_title = f\"<b>{project.get('name', 'Project')}</b>\"\n",
        "            if project.get('technologies'):\n",
        "                project_title += f\" | <i>{project.get('technologies')}</i>\"\n",
        "            story.append(Paragraph(project_title, body_style))\n",
        "\n",
        "            if project.get('description'):\n",
        "                story.append(Paragraph(f\"• {project['description']}\", body_style))\n",
        "\n",
        "            story.append(Spacer(1, 0.1*inch))\n",
        "\n",
        "    # Education\n",
        "    if resume_data.get('education'):\n",
        "        story.append(Paragraph(\"EDUCATION\", section_header_style))\n",
        "        for edu in resume_data['education']:\n",
        "            edu_text = f\"<b>{edu.get('degree', 'Degree')}</b> | {edu.get('institution', 'Institution')}\"\n",
        "            story.append(Paragraph(edu_text, body_style))\n",
        "\n",
        "            edu_details = f\"<i>{edu.get('graduation_date', '')} | GPA: {edu.get('gpa', 'N/A')}</i>\"\n",
        "            story.append(Paragraph(edu_details, body_style))\n",
        "            story.append(Spacer(1, 0.1*inch))\n",
        "\n",
        "    # Build PDF\n",
        "    doc.build(story)\n",
        "    buffer.seek(0)\n",
        "    return buffer\n",
        "\n"
      ],
      "metadata": {
        "id": "_udDm9HLPW2m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask API\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"\"\"\n",
        "    <h1>🚀 Enhanced Resume Generator API with PDF Export</h1>\n",
        "    <p>API is running! Generate professional resumes with AI.</p>\n",
        "    <h3>Endpoints:</h3>\n",
        "    <ul>\n",
        "        <li><b>POST /api/analyze-job</b> - Extract keywords from job description</li>\n",
        "        <li><b>POST /api/generate-resume</b> - Generate complete tailored resume (JSON)</li>\n",
        "        <li><b>POST /api/generate-resume-pdf</b> - Generate and download PDF resume</li>\n",
        "    </ul>\n",
        "    <p>Model: FLAN-T5-Large (Improved Quality)</p>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@app.route(\"/api/analyze-job\", methods=[\"POST\"])\n",
        "def analyze_job():\n",
        "    \"\"\"Extract keywords from job description\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        job_description = data.get(\"job_description\", \"\")\n",
        "\n",
        "        if not job_description:\n",
        "            return jsonify({\"error\": \"job_description is required\"}), 400\n",
        "\n",
        "        keywords = extract_keywords_from_job(job_description)\n",
        "\n",
        "        return jsonify({\n",
        "            \"success\": True,\n",
        "            \"keywords\": keywords,\n",
        "            \"keywords_list\": [k.strip() for k in keywords.split(\",\") if k.strip()]\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "@app.route(\"/api/generate-resume\", methods=[\"POST\"])\n",
        "def generate_resume():\n",
        "    \"\"\"Generate a complete tailored resume (JSON response)\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "\n",
        "        job_description = data.get(\"job_description\", \"\")\n",
        "        personal_info = data.get(\"personal_info\", {})\n",
        "        education = data.get(\"education\", [])\n",
        "        experience = data.get(\"experience\", [])\n",
        "        projects = data.get(\"projects\", [])\n",
        "        skills = data.get(\"skills\", [])\n",
        "\n",
        "        if not job_description:\n",
        "            return jsonify({\"error\": \"job_description is required\"}), 400\n",
        "\n",
        "        # Extract keywords\n",
        "        keywords = extract_keywords_from_job(job_description)\n",
        "        keywords_list = [k.strip() for k in keywords.split(\",\") if k.strip()]\n",
        "\n",
        "        # Generate professional summary\n",
        "        user_background = {\n",
        "            \"experience\": experience,\n",
        "            \"skills\": skills\n",
        "        }\n",
        "        professional_summary = generate_professional_summary(user_background, keywords, job_description)\n",
        "\n",
        "        # Tailor experience bullets\n",
        "        tailored_experience = []\n",
        "        for exp in experience:\n",
        "            tailored_bullets = []\n",
        "            for bullet in exp.get(\"bullets\", []):\n",
        "                # Only tailor if bullet is substantial\n",
        "                if len(bullet) > 10:\n",
        "                    improved = tailor_resume_experience(bullet, keywords)\n",
        "                    tailored_bullets.append(improved)\n",
        "                else:\n",
        "                    tailored_bullets.append(bullet)\n",
        "\n",
        "            tailored_experience.append({\n",
        "                **exp,\n",
        "                \"bullets\": tailored_bullets\n",
        "            })\n",
        "\n",
        "        # Tailor project descriptions\n",
        "        tailored_projects = []\n",
        "        for project in projects:\n",
        "            tailored_desc = project.get(\"description\", \"\")\n",
        "            if len(tailored_desc) > 15:\n",
        "                tailored_desc = tailor_project_description(\n",
        "                    project.get(\"description\", \"\"),\n",
        "                    project.get(\"technologies\", \"\"),\n",
        "                    keywords\n",
        "                )\n",
        "\n",
        "            tailored_projects.append({\n",
        "                **project,\n",
        "                \"description\": tailored_desc\n",
        "            })\n",
        "\n",
        "        # Calculate match score\n",
        "        match_score = 0\n",
        "        if keywords_list and skills:\n",
        "            matched = sum(1 for k in keywords_list if any(k.lower() in s.lower() for s in skills))\n",
        "            match_score = (matched / len(keywords_list)) * 100\n",
        "\n",
        "        resume_data = {\n",
        "            \"success\": True,\n",
        "            \"keywords_extracted\": keywords_list,\n",
        "            \"professional_summary\": professional_summary,\n",
        "            \"personal_info\": personal_info,\n",
        "            \"education\": education,\n",
        "            \"experience\": tailored_experience,\n",
        "            \"projects\": tailored_projects,\n",
        "            \"skills\": skills,\n",
        "            \"match_score\": round(match_score, 1)\n",
        "        }\n",
        "\n",
        "        return jsonify(resume_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "@app.route(\"/api/generate-resume-pdf\", methods=[\"POST\"])\n",
        "def generate_resume_pdf():\n",
        "    \"\"\"Generate and download professional PDF resume\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "\n",
        "        job_description = data.get(\"job_description\", \"\")\n",
        "        personal_info = data.get(\"personal_info\", {})\n",
        "        education = data.get(\"education\", [])\n",
        "        experience = data.get(\"experience\", [])\n",
        "        projects = data.get(\"projects\", [])\n",
        "        skills = data.get(\"skills\", [])\n",
        "\n",
        "        if not job_description:\n",
        "            return jsonify({\"error\": \"job_description is required\"}), 400\n",
        "\n",
        "        # Generate tailored resume content\n",
        "        keywords = extract_keywords_from_job(job_description)\n",
        "        user_background = {\"experience\": experience, \"skills\": skills}\n",
        "        professional_summary = generate_professional_summary(user_background, keywords, job_description)\n",
        "\n",
        "        # Tailor experience\n",
        "        tailored_experience = []\n",
        "        for exp in experience:\n",
        "            tailored_bullets = []\n",
        "            for bullet in exp.get(\"bullets\", []):\n",
        "                if len(bullet) > 10:\n",
        "                    improved = tailor_resume_experience(bullet, keywords)\n",
        "                    tailored_bullets.append(improved)\n",
        "                else:\n",
        "                    tailored_bullets.append(bullet)\n",
        "            tailored_experience.append({**exp, \"bullets\": tailored_bullets})\n",
        "\n",
        "        # Tailor projects\n",
        "        tailored_projects = []\n",
        "        for project in projects:\n",
        "            tailored_desc = project.get(\"description\", \"\")\n",
        "            if len(tailored_desc) > 15:\n",
        "                tailored_desc = tailor_project_description(\n",
        "                    project.get(\"description\", \"\"),\n",
        "                    project.get(\"technologies\", \"\"),\n",
        "                    keywords\n",
        "                )\n",
        "            tailored_projects.append({**project, \"description\": tailored_desc})\n",
        "\n",
        "        # Prepare resume data for PDF\n",
        "        resume_data = {\n",
        "            \"professional_summary\": professional_summary,\n",
        "            \"personal_info\": personal_info,\n",
        "            \"education\": education,\n",
        "            \"experience\": tailored_experience,\n",
        "            \"projects\": tailored_projects,\n",
        "            \"skills\": skills\n",
        "        }\n",
        "\n",
        "        # Generate PDF\n",
        "        pdf_buffer = create_professional_resume_pdf(resume_data)\n",
        "\n",
        "        filename = f\"Resume_{personal_info.get('name', 'User').replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.pdf\"\n",
        "\n",
        "        return send_file(\n",
        "            pdf_buffer,\n",
        "            mimetype='application/pdf',\n",
        "            as_attachment=True,\n",
        "            download_name=filename\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n"
      ],
      "metadata": {
        "id": "eKZyvxnhPg4o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"34RuNCVR0o60Xl34SO01IPOJy0p_eu3CHpGA8gjg8sSYc1hR\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "port = 5000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"✅ API is LIVE!\")\n",
        "print(f\"🌐 Public URL: {public_url}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run Flask app\n",
        "threading.Thread(target=app.run, kwargs={\"port\": port}).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdM8OzNBP1I2",
        "outputId": "3090e1f8-d763-455c-92c9-6008d150cde3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "✅ API is LIVE!\n",
            "🌐 Public URL: https://machinable-skyla-consortable.ngrok-free.dev\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10614f4d",
        "outputId": "946dcf92-c2fd-4f95-bebb-90f4838036f4"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace with the public URL from the ngrok output\n",
        "# Example: api_url = \"https://machinable-skyla-consortable.ngrok-free.dev/\"\n",
        "api_url = \"https://machinable-skyla-consortable.ngrok-free.dev/\"\n",
        "\n",
        "# --- Test /api/analyze-job ---\n",
        "print(\"=\"*50)\n",
        "print(\"Testing /api/analyze-job\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "job_description_data = {\n",
        "    \"job_description\": \"We are looking for a skilled Software Engineer with experience in Python, Django, and AWS to join our team. Responsibilities include developing web applications and contributing to cloud infrastructure.\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(f\"{api_url}/api/analyze-job\", json=job_description_data)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    print(\"Response Status Code:\", response.status_code)\n",
        "    print(\"Response JSON:\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error testing /api/analyze-job: {e}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Test /api/generate-resume ---\n",
        "print(\"=\"*50)\n",
        "print(\"Testing /api/generate-resume (JSON output)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "resume_input_data = {\n",
        "    \"job_description\": \"We are looking for a skilled Software Engineer with experience in Python, Django, and AWS to join our team. Responsibilities include developing web applications and contributing to cloud infrastructure.\",\n",
        "    \"personal_info\": {\n",
        "        \"name\": \"Jane Doe\",\n",
        "        \"email\": \"jane.doe@example.com\",\n",
        "        \"phone\": \"123-456-7890\",\n",
        "        \"location\": \"San Francisco, CA\",\n",
        "        \"linkedin\": \"linkedin.com/in/janedoe\"\n",
        "    },\n",
        "    \"education\": [\n",
        "        {\n",
        "            \"degree\": \"Master of Science in Computer Science\",\n",
        "            \"institution\": \"University of California, Berkeley\",\n",
        "            \"graduation_date\": \"May 2022\",\n",
        "            \"gpa\": \"3.9\"\n",
        "        },\n",
        "        {\n",
        "            \"degree\": \"Bachelor of Science in Electrical Engineering\",\n",
        "            \"institution\": \"Stanford University\",\n",
        "            \"graduation_date\": \"May 2020\",\n",
        "            \"gpa\": \"3.8\"\n",
        "        }\n",
        "    ],\n",
        "    \"experience\": [\n",
        "        {\n",
        "            \"title\": \"Software Engineer\",\n",
        "            \"company\": \"Tech Solutions Inc.\",\n",
        "            \"start_date\": \"June 2022\",\n",
        "            \"end_date\": \"Present\",\n",
        "            \"location\": \"San Francisco, CA\",\n",
        "            \"bullets\": [\n",
        "                \"Developed and maintained web applications using Python and Django.\",\n",
        "                \"Managed cloud infrastructure on AWS.\",\n",
        "                \"Collaborated with cross-functional teams.\"\n",
        "            ]\n",
        "        },\n",
        "         {\n",
        "            \"title\": \"Intern\",\n",
        "            \"company\": \"Data Analytics Corp.\",\n",
        "            \"start_date\": \"Summer 2021\",\n",
        "            \"end_date\": \"August 2021\",\n",
        "            \"location\": \"San Francisco, CA\",\n",
        "            \"bullets\": [\n",
        "                \"Assisted in data analysis projects.\",\n",
        "                \"Wrote scripts in Python.\",\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"projects\": [\n",
        "        {\n",
        "            \"name\": \"E-commerce Platform\",\n",
        "            \"technologies\": \"React, Node.js, MongoDB\",\n",
        "            \"description\": \"Built a full-stack e-commerce platform.\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Sentiment Analysis Tool\",\n",
        "            \"technologies\": \"Python, TensorFlow\",\n",
        "            \"description\": \"Developed a machine learning model for sentiment analysis.\"\n",
        "        }\n",
        "    ],\n",
        "    \"skills\": [\"Python\", \"Django\", \"AWS\", \"React\", \"Node.js\", \"MongoDB\", \"TensorFlow\", \"SQL\", \"Docker\", \"Kubernetes\"]\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(f\"{api_url}/api/generate-resume\", json=resume_input_data)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    print(\"Response Status Code:\", response.status_code)\n",
        "    print(\"Response JSON:\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error testing /api/generate-resume: {e}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Test /api/generate-resume-pdf ---\n",
        "print(\"=\"*50)\n",
        "print(\"Testing /api/generate-resume-pdf (PDF download - requires manual check)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# This endpoint returns a PDF file. We will just check for a successful response.\n",
        "try:\n",
        "    response = requests.post(f\"{api_url}/api/generate-resume-pdf\", json=resume_input_data)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    print(\"Response Status Code:\", response.status_code)\n",
        "    print(\"Response Headers (check for Content-Disposition and Content-Type):\")\n",
        "    for header, value in response.headers.items():\n",
        "        print(f\"  {header}: {value}\")\n",
        "\n",
        "    # You can save the PDF content to a file to verify:\n",
        "    # with open(\"generated_resume.pdf\", \"wb\") as f:\n",
        "    #     f.write(response.content)\n",
        "    # print(\"\\nPDF content received. You can uncomment the lines above to save it.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error testing /api/generate-resume-pdf: {e}\")\n",
        "\n",
        "print(\"\\nTesting complete.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Testing /api/analyze-job\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Oct/2025 08:51:07] \"POST /api/analyze-job HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Status Code: 200\n",
            "Response JSON:\n",
            "{\n",
            "  \"keywords\": \"Engineer, Python, Django, contributing, cloud\",\n",
            "  \"keywords_list\": [\n",
            "    \"Engineer\",\n",
            "    \"Python\",\n",
            "    \"Django\",\n",
            "    \"contributing\",\n",
            "    \"cloud\"\n",
            "  ],\n",
            "  \"success\": true\n",
            "}\n",
            "\n",
            "\n",
            "==================================================\n",
            "Testing /api/generate-resume (JSON output)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Oct/2025 08:51:47] \"POST /api/generate-resume HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Status Code: 200\n",
            "Response JSON:\n",
            "{\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"degree\": \"Master of Science in Computer Science\",\n",
            "      \"gpa\": \"3.9\",\n",
            "      \"graduation_date\": \"May 2022\",\n",
            "      \"institution\": \"University of California, Berkeley\"\n",
            "    },\n",
            "    {\n",
            "      \"degree\": \"Bachelor of Science in Electrical Engineering\",\n",
            "      \"gpa\": \"3.8\",\n",
            "      \"graduation_date\": \"May 2020\",\n",
            "      \"institution\": \"Stanford University\"\n",
            "    }\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"bullets\": [\n",
            "        \"Developed and maintained web applications utilizing Python and Django. Improved bullet point to be professional and impactful.\",\n",
            "        \"Enhanced managed cloud infrastructure on AWS. Improved bullet point to be professional and impactful.\",\n",
            "        \"Enhanced collaborated with cross-functional teams. Improved collaboration speed and efficiency.\"\n",
            "      ],\n",
            "      \"company\": \"Tech Solutions Inc.\",\n",
            "      \"end_date\": \"Present\",\n",
            "      \"location\": \"San Francisco, CA\",\n",
            "      \"start_date\": \"June 2022\",\n",
            "      \"title\": \"Software Engineer\"\n",
            "    },\n",
            "    {\n",
            "      \"bullets\": [\n",
            "        \"Enhanced assisted in data analysis projects. Improved bullet point:\",\n",
            "        \"Enhanced wrote scripts in Python. Improved bullet point to be professional and impactful.\"\n",
            "      ],\n",
            "      \"company\": \"Data Analytics Corp.\",\n",
            "      \"end_date\": \"August 2021\",\n",
            "      \"location\": \"San Francisco, CA\",\n",
            "      \"start_date\": \"Summer 2021\",\n",
            "      \"title\": \"Intern\"\n",
            "    }\n",
            "  ],\n",
            "  \"keywords_extracted\": [\n",
            "    \"Engineer\",\n",
            "    \"Python\",\n",
            "    \"Django\",\n",
            "    \"contributing\",\n",
            "    \"cloud\"\n",
            "  ],\n",
            "  \"match_score\": 40.0,\n",
            "  \"personal_info\": {\n",
            "    \"email\": \"jane.doe@example.com\",\n",
            "    \"linkedin\": \"linkedin.com/in/janedoe\",\n",
            "    \"location\": \"San Francisco, CA\",\n",
            "    \"name\": \"Jane Doe\",\n",
            "    \"phone\": \"123-456-7890\"\n",
            "  },\n",
            "  \"professional_summary\": \"Results-driven Software Engineer with 2+ years of experience leveraging Python and Django to deliver innovative solutions. Proven track record of improving efficiency and driving business outcomes through technical expertise.\",\n",
            "  \"projects\": [\n",
            "    {\n",
            "      \"description\": \"Developed full-stack e-commerce platform with React frontend and Node.js backend, integrating MongoDB payment processing for seamless transactions. Implemented user authentication, product catalog with search/filter functionality, and shopping cart system. Increased customer retention by 45% and improved order fulfillment speed by 50%.\",\n",
            "      \"name\": \"E-commerce Platform\",\n",
            "      \"technologies\": \"React, Node.js, MongoDB\"\n",
            "    },\n",
            "    {\n",
            "      \"description\": \"Developed predictive machine learning model for sentiment analysis using Python and TensorFlow. Performed data preprocessing, feature engineering, and hyperparameter tuning. Deployed model to production, enabling proactive retention strategies that reduced retention by 35%.\",\n",
            "      \"name\": \"Sentiment Analysis Tool\",\n",
            "      \"technologies\": \"Python, TensorFlow\"\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": [\n",
            "    \"Python\",\n",
            "    \"Django\",\n",
            "    \"AWS\",\n",
            "    \"React\",\n",
            "    \"Node.js\",\n",
            "    \"MongoDB\",\n",
            "    \"TensorFlow\",\n",
            "    \"SQL\",\n",
            "    \"Docker\",\n",
            "    \"Kubernetes\"\n",
            "  ],\n",
            "  \"success\": true\n",
            "}\n",
            "\n",
            "\n",
            "==================================================\n",
            "Testing /api/generate-resume-pdf (PDF download)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [23/Oct/2025 08:52:26] \"POST /api/generate-resume-pdf HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Status Code: 200\n",
            "PDF content received and saved to Resume_Jane_Doe_20251023.pdf\n",
            "\n",
            "Testing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the path to the directory you want to download\n",
        "directory_to_download = \"./flan-t5-base-resume-finetuned-stable\"\n",
        "zip_filename = \"flan-t5-base-finetuned-stable.zip\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(directory_to_download):\n",
        "    print(f\"Compressing directory: {directory_to_download}\")\n",
        "    # Create a zip archive of the directory\n",
        "    shutil.make_archive(zip_filename.replace(\".zip\", \"\"), 'zip', directory_to_download)\n",
        "    print(f\"✓ Directory compressed to {zip_filename}\")\n",
        "\n",
        "    # Provide a download link for the zip file\n",
        "    try:\n",
        "        files.download(zip_filename)\n",
        "        print(f\"\\n✓ Download initiated for {zip_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError initiating download: {e}\")\n",
        "        print(f\"You can manually download the file '{zip_filename}' from the Colab file explorer.\")\n",
        "else:\n",
        "    print(f\"❌ Error: Directory not found at {directory_to_download}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "MagY9-1VRv8M",
        "outputId": "81b51267-91cd-4f2f-f399-5ee236f4efd2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressing directory: ./flan-t5-base-resume-finetuned-stable\n",
            "✓ Directory compressed to flan-t5-base-finetuned-stable.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9741d0a9-1afc-4f9c-b168-129f067b05f3\", \"flan-t5-base-finetuned-stable.zip\", 3683190082)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Download initiated for flan-t5-base-finetuned-stable.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhYbj7m-SH9F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}